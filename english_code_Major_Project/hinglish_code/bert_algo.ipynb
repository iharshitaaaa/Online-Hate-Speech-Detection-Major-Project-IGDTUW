{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"hh_O_vZTHBOd"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40750,"status":"ok","timestamp":1710079397014,"user":{"displayName":"144_Divya Pilania","userId":"05641810758440914792"},"user_tz":-330},"id":"jDRXgwNFLp06","outputId":"8dbc3603-05b2-4ecb-854b-cca94eba5ae0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ktrain\n","  Downloading ktrain-0.41.1.tar.gz (25.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.2.2)\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from ktrain) (3.7.1)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.5.3)\n","Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ktrain) (2.31.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.3.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ktrain) (23.2)\n","Collecting langdetect (from ktrain)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from ktrain) (0.42.1)\n","Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (from ktrain) (3.3.2)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from ktrain) (5.2.0)\n","Collecting syntok>1.3.3 (from ktrain)\n","  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n","Collecting tika (from ktrain)\n","  Downloading tika-2.6.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers<=4.37.2 (from ktrain)\n","  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from ktrain) (0.1.99)\n","Collecting keras_bert>=0.86.0 (from ktrain)\n","  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting whoosh (from ktrain)\n","  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_bert>=0.86.0->ktrain) (1.25.2)\n","Collecting keras-transformer==0.40.0 (from keras_bert>=0.86.0->ktrain)\n","  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-pos-embd==0.13.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n","  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-multi-head==0.29.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n","  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-layer-normalization==0.16.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n","  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-position-wise-feed-forward==0.8.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n","  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-embed-sim==0.10.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n","  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-self-attention==0.51.0 (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n","  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->ktrain) (2023.4)\n","Requirement already satisfied: regex>2016 in /usr/local/lib/python3.10/dist-packages (from syntok>1.3.3->ktrain) (2023.12.25)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<=4.37.2->ktrain) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.37.2->ktrain) (0.20.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.37.2->ktrain) (6.0.1)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.37.2->ktrain) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.37.2->ktrain) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.37.2->ktrain) (4.66.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->ktrain) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2024.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika->ktrain) (67.7.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers<=4.37.2->ktrain) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers<=4.37.2->ktrain) (4.10.0)\n","Building wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, tika\n","  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ktrain: filename=ktrain-0.41.1-py3-none-any.whl size=25316877 sha256=d45c768e8977ec926c50a7db5a7b863d1356e8f47f688eae67a56ad42425f615\n","  Stored in directory: /root/.cache/pip/wheels/b7/d8/4d/00cca256dcaa6b3669f92caac7474adbece9c8ecc468ade8cb\n","  Building wheel for keras_bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33499 sha256=4ea508d34ae807310306f5e36bd4f50c9c8d1ba197cfd23efcf6558c0c88808a\n","  Stored in directory: /root/.cache/pip/wheels/89/0c/04/646b6fdf6375911b42c8d540a8a3fda8d5d77634e5dcbe7b26\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12286 sha256=e381249b10f9226766f6f834b3b9fde2102e97fd465849319795723aae22b69e\n","  Stored in directory: /root/.cache/pip/wheels/f2/cb/22/75a0ad376129177f7c95c0d91331a18f5368fd657f4035ba7c\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3943 sha256=df2f4c549beec25e0d4f0268fdd3f38def2f2441238cb380d7f14612271c19ea\n","  Stored in directory: /root/.cache/pip/wheels/82/32/c7/fd35d0d1b840a6c7cbd4343f808d10d0f7b87d271a4dbe796f\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4653 sha256=f9fd74318e0d07dd785a3c703d85c6a820b36ce7bbb47ac5a1d8a5633c29f63f\n","  Stored in directory: /root/.cache/pip/wheels/ed/3a/4b/21db23c0cc56c4b219616e181f258eb7c57d36cc5d056fae9a\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14975 sha256=79ed77d3714f0992c52bdc9215e94a80b7cfadfa0dec1c8918f949fb96d44214\n","  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6946 sha256=bffe836d945ed66330b719df870705e31df5d603b2e145b119bd9b02d82e4c9f\n","  Stored in directory: /root/.cache/pip/wheels/78/07/1b/b1ca47b6ac338554b75c8f52c54e6a2bfbe1b07d79579979a4\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4968 sha256=5b99da03732589ca0e6dab745f95a8c077d396bcaef6bfc4562a1a85a9a9d61e\n","  Stored in directory: /root/.cache/pip/wheels/c1/6a/04/d1706a53b23b2cb5f9a0a76269bf87925daa1bca09eac01b21\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=b3f65fb491eda84d7ae600a7461aec7eafe0898f06181c3d86057c0905deaadb\n","  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=cbd08388a94e093938b1d7ac7d53a005a151e062e18b565a1b077271eff8c83a\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32622 sha256=f570ea35d9ba4368739b7d9eb62eae735b67f925a38fc9a2cbf06d666ac91e46\n","  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n","Successfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect tika\n","Installing collected packages: whoosh, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, tika, keras-multi-head, keras-transformer, transformers, keras_bert, ktrain\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.38.2\n","    Uninstalling transformers-4.38.2:\n","      Successfully uninstalled transformers-4.38.2\n","Successfully installed keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.41.1 langdetect-1.0.9 syntok-1.4.4 tika-2.6.0 transformers-4.37.2 whoosh-2.7.4\n"]}],"source":["!pip3 install ktrain\n","import ktrain\n","from ktrain import text"]},{"cell_type":"markdown","metadata":{"id":"CLMbL1Mx_0_M"},"source":["# Bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rkzuti6MvWe"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import time\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8K2jEuNMYm2"},"outputs":[],"source":["data = pd.read_csv(\"dataset.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2e3Bbr1M13V"},"outputs":[],"source":["# Assuming X and y are your data and target variable, respectively\n","data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZBuMlpuNI3Y"},"outputs":[],"source":["X_train = data_train.comment.tolist()\n","X_test = data_test.comment.tolist()\n","\n","y_train = data_train.isHate.tolist()\n","y_test = data_test.isHate.tolist()\n","\n","data = pd.concat([data_train, data_test], ignore_index=True)\n","\n","class_names = ['hate', 'non hate']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":10190,"status":"ok","timestamp":1710079437275,"user":{"displayName":"144_Divya Pilania","userId":"05641810758440914792"},"user_tz":-330},"id":"U5bCqwUUOAYl","outputId":"2ace62db-a489-410e-b9eb-177057dd005d"},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n","[██████████████████████████████████████████████████]\n","extracting pretrained BERT model...\n","done.\n","\n","cleanup downloaded zip...\n","done.\n","\n","preprocessing train...\n","language: en\n"]},{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["done."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Is Multi-Label? False\n","preprocessing test...\n","language: en\n"]},{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["done."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["task: text classification\n"]}],"source":["(x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=X_train, y_train=y_train,\n","                                                                       x_test=X_test, y_test=y_test,\n","                                                                       class_names=class_names,\n","                                                                       preprocess_mode='bert',\n","                                                                       maxlen=350,\n","                                                                       max_features=35000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7010,"status":"ok","timestamp":1710079449334,"user":{"displayName":"144_Divya Pilania","userId":"05641810758440914792"},"user_tz":-330},"id":"Q9S6WIbQN9F0","outputId":"65f4ce23-f71a-4999-b909-a22164410440"},"outputs":[{"name":"stdout","output_type":"stream","text":["Is Multi-Label? False\n","maxlen is 350\n","done.\n"]}],"source":["model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOI7jlikOKLd"},"outputs":[],"source":["learner = ktrain.get_learner(model, train_data=(x_train, y_train),\n","                             val_data=(x_test, y_test),\n","                             batch_size=6)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"l-iPAmXSOOBe","outputId":"d486e2ef-def9-49c1-eeaa-976c6d640966"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","begin training using onecycle policy with max lr of 2e-05...\n","Epoch 1/5\n","133/133 [==============================] - 95s 579ms/step - loss: 0.6300 - accuracy: 0.6516 - val_loss: 0.5320 - val_accuracy: 0.7350\n","Epoch 2/5\n","133/133 [==============================] - 81s 607ms/step - loss: 0.4657 - accuracy: 0.7782 - val_loss: 0.4600 - val_accuracy: 0.7650\n","Epoch 3/5\n"," 27/133 [=====>........................] - ETA: 55s - loss: 0.1512 - accuracy: 0.9506"]}],"source":["learner.fit_onecycle(2e-5, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2py0jQ0OQA0","outputId":"66f694dc-2f7c-4362-94f5-a2bca2a7ce0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["7/7 [==============================] - 5s 236ms/step\n","              precision    recall  f1-score   support\n","\n","        hate       0.80      0.86      0.83       129\n","    non hate       0.71      0.62      0.66        71\n","\n","    accuracy                           0.78       200\n","   macro avg       0.76      0.74      0.75       200\n","weighted avg       0.77      0.78      0.77       200\n","\n"]},{"data":{"text/plain":["array([[111,  18],\n","       [ 27,  44]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["learner.validate(val_data=(x_test, y_test), class_names=class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idLou5IrOShG","outputId":"3fd45a58-4fe6-440a-c1ac-142f2da4fd95"},"outputs":[{"data":{"text/plain":["['hate', 'non hate']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["predictor = ktrain.get_predictor(learner.model, preproc)\n","predictor.get_classes()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VcDIDKS6OUj3","outputId":"239d9fd3-67be-4504-df2e-11e992bbeb1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["predicted: non hate (0.10)\n"]}],"source":["import time\n","\n","message = 'Well im glad that i live in Serbia, migrants and islamists are not welcome here!'\n","\n","start_time = time.time()\n","prediction = predictor.predict(message)\n","\n","print('predicted: {} ({:.2f})'.format(prediction, (time.time() - start_time)))"]},{"cell_type":"markdown","metadata":{"id":"4rPHVisA_lZn"},"source":["## Bert + LSTM (Hybrid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IowJOhQMAkg5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, TFBertModel\n","from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, GlobalAveragePooling1D, Bidirectional\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from keras.preprocessing.sequence import pad_sequences\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoD2gmhj75JX"},"outputs":[],"source":["# Load the data\n","dataset = pd.read_csv('dataset.csv')\n","\n","# Handle \"sit\" column containing string values\n","def clean_text(text):\n","    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-English characters\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","    return text\n","\n","# Apply preprocessing steps\n","dataset['comment'] = dataset['comment'].apply(clean_text)\n","\n","# Split data into features (X) and target (y)\n","X = dataset['comment']  # Assuming 'comment' contains your text data\n","y = dataset['isHate'].values\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyA8EOmw_9n5","outputId":"fdd2121b-78ed-4090-c881-12b2d653c5d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_20\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_29 (InputLayer)       [(None, 350)]             0         \n","                                                                 \n"," tf_bert_model_18 (TFBertMo  TFBaseModelOutputWithPo   109482240 \n"," del)                        olingAndCrossAttentions             \n","                             (last_hidden_state=(Non             \n","                             e, 350, 768),                       \n","                              pooler_output=(None, 7             \n","                             68),                                \n","                              past_key_values=None,              \n","                             hidden_states=None, att             \n","                             entions=None, cross_att             \n","                             entions=None)                       \n","                                                                 \n"," bidirectional_15 (Bidirect  (None, 350, 128)          426496    \n"," ional)                                                          \n","                                                                 \n"," global_average_pooling1d_1  (None, 128)               0         \n"," 8 (GlobalAveragePooling1D)                                      \n","                                                                 \n"," dense_19 (Dense)            (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 109908865 (419.27 MB)\n","Trainable params: 109908865 (419.27 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n"]}],"source":["# Tokenize input text\n","max_len = 350  # Set your desired maximum sequence length\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","X_encoded = [tokenizer.encode(text, add_special_tokens=True, max_length=max_len, truncation=True) for text in X]\n","\n","# Pad tokenized sequences to ensure uniform length\n","X_padded = pad_sequences(X_encoded, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n","\n","# Define model\n","input_layer_text = Input(shape=(max_len,), dtype=np.int32)\n","bert_embedding = TFBertModel.from_pretrained('bert-base-uncased')(input_layer_text)[0]\n","lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(bert_embedding)\n","pooled_output = GlobalAveragePooling1D()(lstm_layer)\n","output_layer = Dense(1, activation='sigmoid')(pooled_output)  # Output layer for binary classification\n","\n","# Define model\n","model = Model(inputs=input_layer_text, outputs=output_layer)\n","\n","# Print model summary\n","print(model.summary())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jh3CNqEC_9kf","outputId":"79418964-196c-4b89-cdd0-6ae888bc4303"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Training Loss: 0.6881103515625\n","Initial Training Accuracy: 0.602756917476654\n","Epoch 1/5\n","107/107 [==============================] - 65s 231ms/step - loss: 0.6853 - accuracy: 0.6191 - val_loss: 0.6779 - val_accuracy: 0.6313\n","Epoch 2/5\n","107/107 [==============================] - 23s 217ms/step - loss: 0.6564 - accuracy: 0.6411 - val_loss: 0.6583 - val_accuracy: 0.6313\n","Epoch 3/5\n","107/107 [==============================] - 23s 211ms/step - loss: 0.6559 - accuracy: 0.6411 - val_loss: 0.6598 - val_accuracy: 0.6313\n","Epoch 4/5\n","107/107 [==============================] - 22s 209ms/step - loss: 0.6545 - accuracy: 0.6411 - val_loss: 0.6583 - val_accuracy: 0.6313\n","Epoch 5/5\n","107/107 [==============================] - 22s 207ms/step - loss: 0.6542 - accuracy: 0.6411 - val_loss: 0.6584 - val_accuracy: 0.6313\n"]}],"source":["# Compile the model\n","model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Evaluate the model on training data\n","train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n","print(\"Initial Training Loss:\", train_loss)\n","print(\"Initial Training Accuracy:\", train_accuracy)\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=5, batch_size=6, validation_split=0.2, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4pnJO-29V63","outputId":"6e0d2d5b-0535-4b60-86e3-59370ec02fb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["7/7 [==============================] - 5s 233ms/step\n","              precision    recall  f1-score   support\n","\n","        hate       0.65      1.00      0.78       129\n","    non hate       0.00      0.00      0.00        71\n","\n","    accuracy                           0.65       200\n","   macro avg       0.32      0.50      0.39       200\n","weighted avg       0.42      0.65      0.51       200\n","\n"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test)\n","y_pred_classes = y_pred.round().astype(int)\n","\n","# Get class labels\n","class_labels = ['hate', 'non hate']\n","\n","# Print classification report\n","print(classification_report(y_test, y_pred_classes, target_names=class_labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r08PqerC-JKl","outputId":"b14222ad-4101-433f-fd0e-aac95653f44e"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 47ms/step\n","predicted: non hate (0.10)\n"]}],"source":["import time\n","\n","# Function for single prediction\n","def predict_single(message, model, tokenizer, max_len):\n","    # Clean text\n","    cleaned_text = clean_text(message)\n","    # Tokenize input text\n","    input_ids = tokenizer.encode(cleaned_text, add_special_tokens=True, max_length=max_len, truncation=True)\n","    # Pad tokenized sequence\n","    input_ids = pad_sequences([input_ids], maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","    # Make prediction\n","    prediction = model.predict(input_ids)\n","    return prediction[0][0]  # Return the probability of the positive class\n","\n","message = 'Well im glad that i live in Serbia, everyone are welcome here! '\n","start_time = time.time()\n","prediction = predict_single(message, model, tokenizer, max_len)\n","\n","class_name = \"hate\" if prediction > 0.5 else \"non hate\"\n","print('predicted: {} ({:.2f})'.format(class_name, (time.time() - start_time)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJhBRM9I-fJa"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}